root@DESKTOP-771B7RU:/mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo# ls
[0m[34;42m__pycache__[0m           [01;32mhybrid_asp_dqn_training_more_dqns_new.py[0m  [34;42mlogs[0m                [01;32mresult.tmp[0m       [01;32msimple_net.py[0m
[01;32mcmds_hist.txt[0m         [01;32mhybrid_model_atari.py[0m                     [01;32mmontezuma_basic.lp[0m  [34;42mroms[0m             [34;42msummary_v1[0m
[01;32mconstraint.lp[0m         [01;32mhybrid_rl_il_agent_atari.py[0m               [01;32mplanner.py[0m          [01;32mschedules.py[0m     [01;32mtensorboard.py[0m
[01;32mdevice_config.py[0m      [01;32mhyperparameters_new.py[0m                    [01;32mq.lp[0m                [01;32mscreenlog.0[0m      [01;32mvisualizer.py[0m
[01;32menvironment_atari.py[0m  [01;32minitial.lp[0m                                [01;32mreplay_buffer.py[0m    [01;32msegment_tree.py[0m
[01;32mgoal.lp[0m               [01;32minitial_screen.jpeg[0m                       [01;32mrequirements.txt[0m    [01;32mshow.lp[0m
root@DESKTOP-771B7RU:/mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo# lsscreen -xL -helppython3.8 hybrid_asp_dqn_training_mmore_dqns_new.py [A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cclingo ../SDRL_Source/montezuma_bas[10Pic.lp[C[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython3.8 hybrid_asp_dqn_training_m[10@ore_dqns_new.py[C[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[21Pscreen --help
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[3PL [1Pxls[Kscreen -xL -help[3PL [1Pxls[K[Klsscreen -xL -helppython3.8 hybrid_asp_dqn_training_mmore_dqns_new.py [A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cclingo ../SDRL_Source/montezuma_bas[10Pic.lp[C[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Capt-get install gringoclingoclingo[K ../SDRL_Source/montezuma_bassic.lp [A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[3Ppython3.8 -m pip install clingo
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chybrid_asp_dqn_training_mmore_dqns_new.py 
2022-11-26 22:39:00.058873: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-26 22:39:03.832713: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-11-26 22:39:03.832914: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-11-26 22:39:12.152115: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-26 22:39:12.152376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-26 22:39:12.152420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
A.L.E: Arcade Learning Environment (version 0.8.0+919230b)
[Powered by Stella]
size of screen is: (84, 84)
agent loc: (42.333333333333336, 33.333333333333336)
WARNING:tensorflow:From /mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo/device_config.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
WARNING:tensorflow:From /mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo/device_config.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2022-11-26 22:39:21.328131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-26 22:39:21.397575: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:
2022-11-26 22:39:21.397684: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2022-11-26 22:39:21.397706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-771B7RU): /proc/driver/nvidia/version does not exist
[INFO] using device:  cpu
size of screen is: (84, 84)
/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)


### EPISODE 0###
generate new plan...
Generate symbolic plan...
montezuma_basic.lp:29:51-71: info: atom does not occur in any rule head:
  ro(at(L1),move(L),Z)

montezuma_basic.lp:30:51-85: info: atom does not occur in any rule head:
  ro((at(L1),picked(key)),move(L),Z)

montezuma_basic.lp:31:62-82: info: atom does not occur in any rule head:
  ro(at(L1),move(L),Z)

montezuma_basic.lp:32:58-92: info: atom does not occur in any rule head:
  ro((at(L1),picked(key)),move(L),Z)

Find a plan in 2 steps
[94m[TIME STAMP] 1 [0m
[92m[FLUENTS][0m at(plat1) cost(0) 
[92m[ACTIONS][0m [1mmove(key) [0m
[94m[TIME STAMP] 2 [0m
[92m[FLUENTS][0m at(key) picked(key) cost(50) 
PLAN TRACE:   [(1, 'move(key) ', 'at(plat1) cost(0) '), (2, '', 'at(key) picked(key) cost(50) ')]
Subgoal not found for  at(key) picked(key) cost(50) 
Untrainable symbolic actions.
R( -1 2 )= -20.0
ro( -1 2 )= -100.0
plan quality is: -100
An action in this plan is abandoned. Exploration must start
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 1###
generate new plan...
Generate symbolic plan...
Find a plan in 2 steps
[94m[TIME STAMP] 1 [0m
[92m[FLUENTS][0m at(plat1) cost(0) 
[92m[ACTIONS][0m [1mmove(lower_right_ladder) [0m
[94m[TIME STAMP] 2 [0m
[92m[FLUENTS][0m at(lower_right_ladder) cost(50) 
PLAN TRACE:   [(1, 'move(lower_right_ladder) ', 'at(plat1) cost(0) '), (2, '', 'at(lower_right_ladder) cost(50) ')]
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 1
R( -1 0 )= -1.0
ro( -1 0 )= -5.0
plan quality is: -5
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 2###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 2
R( -1 0 )= -1.4
ro( -1 0 )= -7.5
plan quality is: -8
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 3###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 3
R( -1 0 )= -1.51
ro( -1 0 )= -8.75
plan quality is: -9
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 4###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 4
R( -1 0 )= -1.484
ro( -1 0 )= -9.375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 5###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 5
R( -1 0 )= -1.3981
ro( -1 0 )= -9.6875
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 6###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 6
R( -1 0 )= -1.28954
ro( -1 0 )= -9.84375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 7###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 7
R( -1 0 )= -1.176211
ro( -1 0 )= -9.921875
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 8###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 8
R( -1 0 )= -1.0664023999999999
ro( -1 0 )= -9.9609375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 9###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 9
R( -1 0 )= -0.9636684099999999
ro( -1 0 )= -9.98046875
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 10###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 10
R( -1 0 )= -0.8692546939999999
ro( -1 0 )= -9.990234375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 11###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 11
R( -1 0 )= -0.7833057870999999
ro( -1 0 )= -9.9951171875
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 12###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 12
R( -1 0 )= -0.7054634896399998
ro( -1 0 )= -9.99755859375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 13###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 13
R( -1 0 )= -0.6351612813009998
ro( -1 0 )= -9.998779296875
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 14###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 14
R( -1 0 )= -0.5717672234833998
ro( -1 0 )= -9.9993896484375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 15###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 15
R( -1 0 )= -0.5146515362913098
ro( -1 0 )= -9.99969482421875
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 16###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
Goal not achieved.
Failure times: 16
R( -1 0 )= -0.4632169002403038
ro( -1 0 )= -9.999847412109375
plan quality is: -10
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 17###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
previous state at(plat1) cost(0) 
goal reached at(lower_right_ladder) cost(50) 
Success times: 1
0
R( -1 0 )= 1.5830895309946644
ro( -1 0 )= -0.7914684715520188
now setting episodeSteps to be 0
plan quality is: -1
trying to train subgoal DQN. Continue executing the same plan
Trailing success ratio for 0 is: 0.0
Trailing success ratio for 1 is: 0.0
Trailing success ratio for 2 is: 0.0
Trailing success ratio for 3 is: 0.0
Trailing success ratio for 4 is: 0.0
Trailing success ratio for 5 is: 0.0
Trailing success ratio for 6 is: 0.0
perform annealing


### EPISODE 18###
continue executing previous plan...
current state and action: at(plat1) cost(0)  -1 at(plat1) cost(0)  0
predicted subgoal is:  at(lower_right_ladder) cost(50) 
goal explain lower right ladder
not kick off low level training yet
Finally, the number of stuff in random_experience is 1093
The number of item in experience memory so far is: 1093
This should really be one time thing
 number of option_t is  1093
1/4 [======>.......................] - ETA: 4s4/4 [==============================] - 2s 14ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 10ms/step
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
Perform training on experience replay
loss: 0.17804262042045593 avgQ: 0.067817636 avgTDError 0.49818093894282356
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 7ms/step
Perform training on experience replay
loss: 0.6845433712005615 avgQ: 2.0698254 avgTDError 2.786789678488276
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 7ms/step
Perform training on experience replay
loss: 0.17787417769432068 avgQ: 0.620638 avgTDError 1.0546184833228835
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 9ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 9ms/step
Perform training on experience replay
loss: 0.064419224858284 avgQ: 0.16160232 avgTDError 0.490479471940489
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 8ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 9ms/step
Perform training on experience replay
loss: 0.01776278391480446 avgQ: -0.061838236 avgTDError 0.25411604688633815
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 7ms/step
Perform training on experience replay
loss: 0.00855766236782074 avgQ: -0.17408246 avgTDError 0.2357133482610152
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
Perform training on experience replay
loss: 0.006414047908037901 avgQ: -0.17971218 avgTDError 0.18931806412865626
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 9ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
Perform training on experience replay
loss: 0.015230976976454258 avgQ: -0.10582061 avgTDError 0.3049473523474262
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 7ms/step
Perform training on experience replay
loss: 0.008612530305981636 avgQ: -0.029679572 avgTDError 0.23615714650804875
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 7ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
Perform training on experience replay
loss: 0.005227869842201471 avgQ: -0.17398956 avgTDError 0.19879678613187934
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
Perform training on experience replay
loss: 0.007951999083161354 avgQ: -0.082956836 avgTDError 0.23160450287196
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 6ms/step
1/4 [======>.......................] - ETA: 0s4/4 [==============================] - 0s 7ms/step
^CTraceback (most recent call last):
  File "hybrid_asp_dqn_training_more_dqns_new.py", line 441, in <module>
    main()
  File "hybrid_asp_dqn_training_more_dqns_new.py", line 271, in main
    loss, avgQ, avgTDError = agent_list[goal].update(option_t[goal])
  File "/mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo/hybrid_rl_il_agent_atari.py", line 196, in update
    loss = self._update(stepCount)
  File "/mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo/hybrid_rl_il_agent_atari.py", line 186, in _update
    loss = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets], sample_weight = [np.array(importanceVector), np.ones(self.nSamples)])
  File "/usr/local/lib/python3.8/dist-packages/keras/engine/training.py", line 2478, in train_on_batch
    logs = self.train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py", line 880, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py", line 912, in _call
    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py", line 134, in __call__
^C  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py", line 378, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
^C
root@DESKTOP-771B7RU:/mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo# [Kroot@DESKTOP-771B7RU:/mnt/d/ASU/CSE574/SDRL/symbolic_planning_and_rl/src/mntz_clingo# 